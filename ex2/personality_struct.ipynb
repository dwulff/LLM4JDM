{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Intro\n",
    "In this exercise, we will be using the same feature extraction method from the last to encode the text for the personality items of the NEO Big 5 questionnaire. We will then use cosine similarity between the encoded items to predict the correlations between the personality constructs to which they belong. We will then compare these predicted correlations to the observed correlations between constructs based on a large data set of participant responses to the items."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    # Installing packages in Google Colab environment\n",
    "    !pip install datasets transformers\n",
    "\n",
    "    # Mounting google drive to enable access to data files\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # Changing working directory to ex1\n",
    "    %cd /content/drive/MyDrive/LLM4JDM/ex2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You may notice that the \"Preparing data\" section and \"feature extraction\" section take exactly the same structure as the last notebook. This illustrates the generalizability of the approach. We can use the same code to extract features from any text data, regardless of the specific task we are interested in. The only thing that changes is the data we load and the model we use."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preparing data\n",
    "We again begin by loading the requisite packages. We again make use of the following packages:\n",
    "1. ```pandas```: A very popular package for reading and manipulating data in python.\n",
    "2. ```datasets```: A HuggingFace (HF) package for loading and manipulating datasets in a format ready for use with HF models.\n",
    "3. ```transformers```: A HF package for loading and manipulating transformer-based models."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Out item data has two columns:\n",
    "1. ```construct```: The personality construct to which the item belongs.\n",
    "2. ```text```: The item description."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Loading data with pandas\n",
    "neo_items =  pd.read_csv('NEO_items.csv', usecols=['construct', 'text'])\n",
    "neo_items"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Converting into a HuggingFace dataset\n",
    "dat = Dataset.from_pandas(neo_items)\n",
    "dat"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Loading the tokenizer\n",
    "model_ckpt = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "print(f'Vocabulary size: {tokenizer.vocab_size}, max context length: {tokenizer.model_max_length}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Tokenizing the text\n",
    "tokenize = lambda x: tokenizer(x['text'], padding=True, truncation=True)\n",
    "dat = dat.map(tokenize, batched=True, batch_size=None)\n",
    "print(tokenizer.decode(dat[0]['input_ids']))\n",
    "dat"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature Extraction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Setting the format of the dataset to torch tensors for passing to the model\n",
    "dat.set_format('torch', columns=['input_ids', 'attention_mask'])\n",
    "dat"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Loading the model and moving it to the GPU if available\n",
    "if torch.cuda.is_available():  # for nvidia GPUs\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available(): # for Apple Metal Performance Sharder (mps) GPUs\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "device"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Loading the model\n",
    "model = AutoModel.from_pretrained(model_ckpt).to(device)\n",
    "f'Model inputs: {tokenizer.model_input_names}'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def extract_features(batch):\n",
    "    \"\"\"Extract features from a batch of items\"\"\"\n",
    "    inputs = {k:v.to(device) for k, v in batch.items() if k in tokenizer.model_input_names}\n",
    "    with torch.no_grad():\n",
    "        last_hidden_state = model(**inputs).last_hidden_state\n",
    "        return {\"hidden_state\": last_hidden_state[:,0].cpu().numpy()}\n",
    "\n",
    "\n",
    "dat = dat.map(extract_features, batched=True, batch_size=8)\n",
    "dat"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Comparing Predicted and Observed Construct Similarities"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Numpy is a popular package for scientific computing in python. We will only use it here for its ```triu_indices``` function, which returns the indices of the upper triangle of a matrix."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Converting the hidden state into a data frame for easy manipulation\n",
    "embeds = pd.DataFrame(dat['hidden_state'])\n",
    "embeds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Adding the construct that each embedding represents\n",
    "embeds['construct'] = neo_items['construct']\n",
    "\n",
    "# Calculating the mean embedding for each construct\n",
    "construct_embeds = embeds.groupby('construct').mean(numeric_only=True)\n",
    "construct_embeds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculating the cosine similarity between construct embeddings\n",
    "predicted = pd.DataFrame(\n",
    "    cosine_similarity(construct_embeds), # cosine similarity between each pair of rows\n",
    "    index=construct_embeds.index, # row names\n",
    "    columns=construct_embeds.index # column names\n",
    ")\n",
    "predicted"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "'Neo_correlations.csv' has three columns:\n",
    "1. ```construct_1```: The first construct in the pair.\n",
    "2. ```construct_2```: The second construct in the pair.\n",
    "3. ```correlation```: The empirical correlation between the two constructs."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Loading observed correlations and pivoting to a correlation matrix\n",
    "observed = pd.read_csv('NEO_correlations.csv')\n",
    "observed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pivoting the data transforms it from long to wide format. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Pivoting to a correlation matrix for easy comparison with predicted correlations\n",
    "observed = observed.pivot(index='construct_1', columns='construct_2', values='correlation')\n",
    "observed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Aligning rows and columns the predicted and observed correlations\n",
    "predicted, observed = predicted.align(observed)\n",
    "\n",
    "# printing the column names showing the orders are now the same\n",
    "print(predicted.columns.tolist()) \n",
    "print(observed.columns.tolist())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We next take the lower triangle (excluding the diagonal) of the predicted and observed correlation matrices and flatten them into vectors. We then calculate the correlation between the predicted and observed correlations. This ensures we don't double count the correlations (e.g., the correlation between A and B is the same as the correlation between B and A) and that we don't include the correlation between a construct and itself (which is always 1)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def lower_triangle_flat(df):\n",
    "    \"\"\"Takes the lower triangle of a dataframe and flattens it into a vector\"\"\"\n",
    "    rows, cols = np.triu_indices(len(df), k=1)  # k=1 to exclude the diagonal (self-similarities)\n",
    "    return pd.Series(df.values[rows, cols])\n",
    "\n",
    "predicted, observed = lower_triangle_flat(predicted), lower_triangle_flat(observed)\n",
    "\n",
    "# Correlation between predicted and observed\n",
    "print(f'r: {predicted.corr(observed).round(2)}')\n",
    "print(f'r of absolute values: {predicted.abs().corr(observed.abs()).round(2)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conclusion\n",
    "It seems we can explain a substantial portion of the inter-construct relationship based purely on the semantic information in the items (absolute values $r=.32). Why do you think that is? ;)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
