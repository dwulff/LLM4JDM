{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Intro\n",
    "So far, we have been dealing with encoder models. Encoder models are good at producing features that capture the semantic content of text. However, they are not good at generating text, because of the bi-directional attention mechanism in these models also attending to tokens after a given token. To generate text, attention needs to be masked such that only words preceding a masked token are attended. This is called *causal* language modeling, and is typically implemented by decoder models. In this exercise, we will see how to use causal language models to generate responses to the Cognitive Reflection Test."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    # Installing packages in Google Colab environment\n",
    "    !pip install transformers\n",
    "\n",
    "    # Mounting google drive to enable access to data files\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # Changing working directory to ex1\n",
    "    %cd /content/drive/MyDrive/LLM4JDM/ex3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The transformers ```pipeline()``` function simplifies classic NLP tasks such as text classification, named entity recognition, question answering, summarization, translation, and, and we will see here, text generation into just a few lines of code."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initializing the pipeline\n",
    "generator = pipeline(\"text-generation\", model='gpt2')\n",
    "generator"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now save the three classic questions from the cognitive reflection test as a list of strings."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    'A bat and a ball cost $1.10 in total. The bat costs $1.00 more than the ball. How much does the ball cost?',\n",
    "    'If it takes 5 machines 5 minutes to make 5 widgets, how long would it take 100 machines to make 100 widgets?',\n",
    "    'In a lake, there is a patch of lily pads. Every day, the patch doubles in size. If it takes 48 days for the patch to cover the entire lake, how long would it take for the patch to cover half of the lake?'\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now use the generator to produce a response based on each prompt. This takes:\n",
    "1. ```prompts```: the list of prompts we defined above.\n",
    "2. ```max_length```: Sets an upper bound on the length of the generated text."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Generating outputs\n",
    "outputs = generator(prompts, max_length=100)\n",
    "\n",
    "# Printing\n",
    "for prompt, output in zip(prompts, outputs):\n",
    "    print('-----------------------------')\n",
    "    print('\\033[1m' + prompt + '\\033[0m')\n",
    "    print('-----------------------------')\n",
    "\n",
    "    # Getting the generated text output from the output dictionary\n",
    "    output = output[0]['generated_text']\n",
    "\n",
    "    # Dropping the prompt from the output for nicer formatting\n",
    "    print(output.replace(prompt, '').strip())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Exercises*:\n",
    "1. Based on your understanding of transformers, why do you think a model such as GPT performs so badly on this test?\n",
    "2. Try editing the prompt to give GPT-2 clues. Does it help?\n",
    "3. Try and implement an API call to gpt3.5 or gpt4 on your own.\n",
    "4. Try to set up repeated calls to the API (e.g. using a loop). This would be useful for feeding multiple text samples into the model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conclusion\n",
    "You probably have observed that smaller, less fine-tuned models, such as GPT-2, are capable of producing coherent text but that this text often fails to provide an appropriate answer to the prompt, sometimes in a hilarious way. Larger, more fine-tune models, such as GPT-3.5 or GPT-4, on the other hand, likely have produced very appropriate and accurate answers. We hope that both these failures and successes have given you a better \"theory of mind\" for language models."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
