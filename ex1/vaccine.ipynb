{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # Installing packages in Google Colab environment\n",
    "# !pip install datasets transformers evaluate\n",
    "# !pip install accelerate -U\n",
    "#\n",
    "# # Mounting google drive to enable access to data files\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "#\n",
    "# # Changing working directory to ex1\n",
    "# %cd /content/drive/MyDrive/LLM4JDM/ex1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preparing data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We begin by loading the requisite packages. For those coming from R, packages in Python are sometimes given shorter names for use in the code via the ```import <name> as <nickname>``` syntax (e.g. ```import pandas as pd```). These are usually standardized nicknames. We here make use of three packages:\n",
    "1. ```pandas```: A very popular library for reading and manipulating data in python.\n",
    "2. ```datasets```: A package from the HuggingFace (HF) ecosystem for loading and manipulating datasets in a format ready for use with HF models.\n",
    "3. ```transformers```: A package from the HF ecosystem for loading and manipulating transfomer-based models."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T11:51:37.311859Z",
     "start_time": "2023-08-09T11:51:35.528011Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "      labels                                               text\n0      0.625  Looking at the probability of certain negative...\n1      0.750  Not really, I looked at all the numbers and ma...\n2      0.750  I weighed the side effects with the benefits a...\n3      0.375  Percentages were important. In all things (med...\n4      0.250  I would look up the potential side effects and...\n...      ...                                                ...\n1046   0.750  I looked primarily at the effectiveness of eac...\n1047   0.125                        Personal experience talking\n1048   0.750                         Biased against the vaccine\n1049   0.250  I have had a lot of people whos gotten the vac...\n1050   0.000  My initial choice was to wait and not rush for...\n\n[1051 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>labels</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.625</td>\n      <td>Looking at the probability of certain negative...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.750</td>\n      <td>Not really, I looked at all the numbers and ma...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.750</td>\n      <td>I weighed the side effects with the benefits a...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.375</td>\n      <td>Percentages were important. In all things (med...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.250</td>\n      <td>I would look up the potential side effects and...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1046</th>\n      <td>0.750</td>\n      <td>I looked primarily at the effectiveness of eac...</td>\n    </tr>\n    <tr>\n      <th>1047</th>\n      <td>0.125</td>\n      <td>Personal experience talking</td>\n    </tr>\n    <tr>\n      <th>1048</th>\n      <td>0.750</td>\n      <td>Biased against the vaccine</td>\n    </tr>\n    <tr>\n      <th>1049</th>\n      <td>0.250</td>\n      <td>I have had a lot of people whos gotten the vac...</td>\n    </tr>\n    <tr>\n      <th>1050</th>\n      <td>0.000</td>\n      <td>My initial choice was to wait and not rush for...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1051 rows Ã— 2 columns</p>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vaccine = pd.read_csv('vaccine.csv')\n",
    "vaccine"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T12:10:55.389013Z",
     "start_time": "2023-08-09T12:10:55.373770Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['labels', 'text'],\n    num_rows: 1051\n})"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting pandas dataframe to HF Dataset\n",
    "vaccine = Dataset.from_pandas(vaccine)\n",
    "vaccine"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T12:10:55.988865Z",
     "start_time": "2023-08-09T12:10:55.985919Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Features of the ```Dataset``` object can be accessed like keys in a dictionary, and behave like python lists. Sample can be accessed by index, returning a dictionary where keys correspond to feature names. This reflects the fact that ```Datasets``` is based on Apache Arrow, which defines a typed columnar format that is more memory efficient than native Python"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "{'labels': 0.625,\n 'text': 'Looking at the probability of certain negative side effects and seeing if the vaccine genuinely protects someone would be very important.'}"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vaccine[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T12:15:18.243351Z",
     "start_time": "2023-08-09T12:15:18.237807Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To use models in the HF ecosystem, one must first define a model checkpoint (```ckpt```): the set of weights that are loaded into a given transformer architecture. This often needs to be done well before we even initialise the model, since data preprocessing steps such as tokenization must follow the steps used to train the original model. We just need a pretrained base model for our purposes (i.e. one that has not yet been fine-tuned on a specific task). One popular lightweight option is ```distilbert-base-uncased```."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# Defining model checkpoint\n",
    "model_ckpt = 'distilbert-base-uncased'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T12:15:28.943475Z",
     "start_time": "2023-08-09T12:15:28.940440Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tokenization is the process of breaking raw text into the desired atomic units for one's modelling task. This may be as simple as splitting the text into individual characters. In the case of transformer-based models, tokenization is a bit more complex, usually occurring at the sub-word level."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 30522, max context length: 512\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing the dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "print(f'Vocabulary size: {tokenizer.vocab_size}, max context length: {tokenizer.model_max_length}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T12:20:45.848992Z",
     "start_time": "2023-08-09T12:20:45.561964Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Two important arguments relating to tokenization:\n",
    "1. ```padding```: Used to fill up sequences to a certain length, ensuring that all sequences in a batch have the same length. This is essential for training and inference with deep learning models that operate on fixed-size input tensors.\n",
    "2. ```truncation```: Truncation is the process of cutting off parts of the sequence to ensure that it fits within a specified maximum length (e.g. 512 tokens for BERT models)\n",
    "\n",
    "The combination of padding and truncation ensures that all sequences in a batch conform to a consistent, fixed size, which is essential for processing in parallel on modern hardware like GPUs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/1051 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2971c67c6a804ece8020e8ada8a9784b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "{'labels': tensor(0.6250),\n 'input_ids': tensor([  101,  2559,  2012,  1996,  9723,  1997,  3056,  4997,  2217,  3896,\n          1998,  3773,  2065,  1996, 17404, 15958, 18227,  2619,  2052,  2022,\n          2200,  2590,  1012,   102,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0]),\n 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0])}"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize = lambda batch: tokenizer(batch['text'], padding=\"max_length\", truncation=True)\n",
    "vaccine = vaccine.map(tokenize, batched=True)\n",
    "vaccine[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T12:32:48.697601Z",
     "start_time": "2023-08-09T12:32:48.576657Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Looking at the first sample, we see some important special tokens:\n",
    "1. ```[CLS]```: Often added at the beginning of the input sequence. In the context of classification tasks, the embedding corresponding to the [CLS] token (after passing through the model) is often used as the aggregate representation for the entire sequence.\n",
    "2. ```[SEP]```: Used to separate different segments in a sequence. For example, in tasks that take two different sentences as input (such as question-answering or text-pair classification), the [SEP] token is placed between the two sentences to indicate that they are distinct segments. This helps the model understand and process the two segments appropriately, recognizing the boundaries between them."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "['[CLS]',\n 'looking',\n 'at',\n 'the',\n 'probability',\n 'of',\n 'certain',\n 'negative',\n 'side',\n 'effects',\n 'and',\n 'seeing',\n 'if',\n 'the',\n 'vaccine',\n 'genuinely',\n 'protects',\n 'someone',\n 'would',\n 'be',\n 'very',\n 'important',\n '.',\n '[SEP]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]']"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspecting tokenization\n",
    "tokenizer.convert_ids_to_tokens(vaccine[0]['input_ids'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T12:34:20.212652Z",
     "start_time": "2023-08-09T12:34:20.203916Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "{'labels': tensor(0.6250),\n 'input_ids': tensor([  101,  2559,  2012,  1996,  9723,  1997,  3056,  4997,  2217,  3896,\n          1998,  3773,  2065,  1996, 17404, 15958, 18227,  2619,  2052,  2022,\n          2200,  2590,  1012,   102,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0]),\n 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0])}"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vaccine.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "vaccine"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T12:32:36.698119Z",
     "start_time": "2023-08-09T12:32:34.921334Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature Extraction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(42)\n",
    "from transformers import AutoModel"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-07T16:49:10.657744Z",
     "start_time": "2023-08-07T16:49:10.645428Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='mps')"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the model and moving it to the GPU if available ('cuda' for nvidia GPUs and 'mps' for Apple's Metal Performance Shaders)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-07T16:49:11.067340Z",
     "start_time": "2023-08-07T16:49:11.044093Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/968 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a9297265989b4797a0965e47295541fa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "          0         1         2         3         4         5         6    \\\n0    0.086551 -0.138291 -0.136351 -0.044146 -0.006536 -0.196219  0.129933   \n1   -0.086077 -0.046284  0.170872 -0.090205 -0.148626 -0.077882  0.115379   \n2    0.040985 -0.031094  0.085166 -0.015342 -0.150943 -0.062235  0.269550   \n3   -0.088134 -0.037886  0.070339  0.060876 -0.184012  0.031210  0.169997   \n4    0.069829  0.019595 -0.001682 -0.246711 -0.061300 -0.168547  0.322209   \n..        ...       ...       ...       ...       ...       ...       ...   \n963  0.122108 -0.101899  0.092345 -0.037650 -0.153381 -0.111409  0.147186   \n964  0.060096 -0.027563 -0.197975 -0.167290 -0.256647 -0.193959  0.278221   \n965 -0.264745 -0.118108 -0.243033 -0.065164 -0.172901 -0.085691  0.123498   \n966  0.214027  0.031197 -0.053656 -0.248237 -0.046856 -0.366515  0.371055   \n967  0.311766 -0.014036 -0.040445 -0.234838 -0.193043 -0.164190  0.229046   \n\n          7         8         9    ...       758       759       760  \\\n0    0.005525  0.046519 -0.363784  ... -0.094813 -0.216771 -0.098917   \n1    0.122023  0.147362 -0.176775  ...  0.094826 -0.011131  0.072211   \n2   -0.149736  0.259255 -0.130736  ... -0.165882 -0.097996 -0.081691   \n3    0.052630  0.159975 -0.296517  ... -0.023511 -0.199103  0.055980   \n4    0.217674  0.237314 -0.358912  ...  0.042846  0.005196  0.082999   \n..        ...       ...       ...  ...       ...       ...       ...   \n963  0.085400  0.108409 -0.179295  ... -0.119313 -0.141489 -0.058733   \n964 -0.008111  0.037555 -0.141178  ... -0.165374 -0.132563 -0.002246   \n965  0.065335 -0.042342 -0.166539  ... -0.049490  0.014468 -0.110201   \n966  0.387127 -0.086699 -0.266235  ... -0.101303 -0.187162  0.108514   \n967  0.279721  0.126862 -0.455163  ... -0.002532 -0.214854  0.003515   \n\n          761       762       763       764       765       766       767  \n0    0.025280  0.003007  0.224140 -0.215962 -0.277577  0.211385  0.269926  \n1   -0.027861 -0.058841  0.019084 -0.260078 -0.105455  0.249027  0.535908  \n2   -0.126770  0.114904  0.140170 -0.236507 -0.087407  0.400360  0.436200  \n3   -0.137853  0.082254  0.127104 -0.297239 -0.143943  0.213958  0.313107  \n4   -0.093391  0.086103 -0.116669 -0.277758 -0.230386  0.221281  0.411202  \n..        ...       ...       ...       ...       ...       ...       ...  \n963  0.005137  0.079066  0.090325 -0.175999 -0.101646  0.329370  0.292443  \n964  0.049726  0.072532 -0.039805 -0.274175  0.038117  0.321610  0.417477  \n965 -0.100587  0.256933 -0.017435 -0.034038 -0.193906  0.303171  0.435974  \n966 -0.177368  0.260718  0.150828 -0.205036 -0.006340  0.436779  0.446601  \n967 -0.244796  0.235992  0.053680 -0.070399  0.062612  0.513922  0.477407  \n\n[968 rows x 768 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>758</th>\n      <th>759</th>\n      <th>760</th>\n      <th>761</th>\n      <th>762</th>\n      <th>763</th>\n      <th>764</th>\n      <th>765</th>\n      <th>766</th>\n      <th>767</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.086551</td>\n      <td>-0.138291</td>\n      <td>-0.136351</td>\n      <td>-0.044146</td>\n      <td>-0.006536</td>\n      <td>-0.196219</td>\n      <td>0.129933</td>\n      <td>0.005525</td>\n      <td>0.046519</td>\n      <td>-0.363784</td>\n      <td>...</td>\n      <td>-0.094813</td>\n      <td>-0.216771</td>\n      <td>-0.098917</td>\n      <td>0.025280</td>\n      <td>0.003007</td>\n      <td>0.224140</td>\n      <td>-0.215962</td>\n      <td>-0.277577</td>\n      <td>0.211385</td>\n      <td>0.269926</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.086077</td>\n      <td>-0.046284</td>\n      <td>0.170872</td>\n      <td>-0.090205</td>\n      <td>-0.148626</td>\n      <td>-0.077882</td>\n      <td>0.115379</td>\n      <td>0.122023</td>\n      <td>0.147362</td>\n      <td>-0.176775</td>\n      <td>...</td>\n      <td>0.094826</td>\n      <td>-0.011131</td>\n      <td>0.072211</td>\n      <td>-0.027861</td>\n      <td>-0.058841</td>\n      <td>0.019084</td>\n      <td>-0.260078</td>\n      <td>-0.105455</td>\n      <td>0.249027</td>\n      <td>0.535908</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.040985</td>\n      <td>-0.031094</td>\n      <td>0.085166</td>\n      <td>-0.015342</td>\n      <td>-0.150943</td>\n      <td>-0.062235</td>\n      <td>0.269550</td>\n      <td>-0.149736</td>\n      <td>0.259255</td>\n      <td>-0.130736</td>\n      <td>...</td>\n      <td>-0.165882</td>\n      <td>-0.097996</td>\n      <td>-0.081691</td>\n      <td>-0.126770</td>\n      <td>0.114904</td>\n      <td>0.140170</td>\n      <td>-0.236507</td>\n      <td>-0.087407</td>\n      <td>0.400360</td>\n      <td>0.436200</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.088134</td>\n      <td>-0.037886</td>\n      <td>0.070339</td>\n      <td>0.060876</td>\n      <td>-0.184012</td>\n      <td>0.031210</td>\n      <td>0.169997</td>\n      <td>0.052630</td>\n      <td>0.159975</td>\n      <td>-0.296517</td>\n      <td>...</td>\n      <td>-0.023511</td>\n      <td>-0.199103</td>\n      <td>0.055980</td>\n      <td>-0.137853</td>\n      <td>0.082254</td>\n      <td>0.127104</td>\n      <td>-0.297239</td>\n      <td>-0.143943</td>\n      <td>0.213958</td>\n      <td>0.313107</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.069829</td>\n      <td>0.019595</td>\n      <td>-0.001682</td>\n      <td>-0.246711</td>\n      <td>-0.061300</td>\n      <td>-0.168547</td>\n      <td>0.322209</td>\n      <td>0.217674</td>\n      <td>0.237314</td>\n      <td>-0.358912</td>\n      <td>...</td>\n      <td>0.042846</td>\n      <td>0.005196</td>\n      <td>0.082999</td>\n      <td>-0.093391</td>\n      <td>0.086103</td>\n      <td>-0.116669</td>\n      <td>-0.277758</td>\n      <td>-0.230386</td>\n      <td>0.221281</td>\n      <td>0.411202</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>963</th>\n      <td>0.122108</td>\n      <td>-0.101899</td>\n      <td>0.092345</td>\n      <td>-0.037650</td>\n      <td>-0.153381</td>\n      <td>-0.111409</td>\n      <td>0.147186</td>\n      <td>0.085400</td>\n      <td>0.108409</td>\n      <td>-0.179295</td>\n      <td>...</td>\n      <td>-0.119313</td>\n      <td>-0.141489</td>\n      <td>-0.058733</td>\n      <td>0.005137</td>\n      <td>0.079066</td>\n      <td>0.090325</td>\n      <td>-0.175999</td>\n      <td>-0.101646</td>\n      <td>0.329370</td>\n      <td>0.292443</td>\n    </tr>\n    <tr>\n      <th>964</th>\n      <td>0.060096</td>\n      <td>-0.027563</td>\n      <td>-0.197975</td>\n      <td>-0.167290</td>\n      <td>-0.256647</td>\n      <td>-0.193959</td>\n      <td>0.278221</td>\n      <td>-0.008111</td>\n      <td>0.037555</td>\n      <td>-0.141178</td>\n      <td>...</td>\n      <td>-0.165374</td>\n      <td>-0.132563</td>\n      <td>-0.002246</td>\n      <td>0.049726</td>\n      <td>0.072532</td>\n      <td>-0.039805</td>\n      <td>-0.274175</td>\n      <td>0.038117</td>\n      <td>0.321610</td>\n      <td>0.417477</td>\n    </tr>\n    <tr>\n      <th>965</th>\n      <td>-0.264745</td>\n      <td>-0.118108</td>\n      <td>-0.243033</td>\n      <td>-0.065164</td>\n      <td>-0.172901</td>\n      <td>-0.085691</td>\n      <td>0.123498</td>\n      <td>0.065335</td>\n      <td>-0.042342</td>\n      <td>-0.166539</td>\n      <td>...</td>\n      <td>-0.049490</td>\n      <td>0.014468</td>\n      <td>-0.110201</td>\n      <td>-0.100587</td>\n      <td>0.256933</td>\n      <td>-0.017435</td>\n      <td>-0.034038</td>\n      <td>-0.193906</td>\n      <td>0.303171</td>\n      <td>0.435974</td>\n    </tr>\n    <tr>\n      <th>966</th>\n      <td>0.214027</td>\n      <td>0.031197</td>\n      <td>-0.053656</td>\n      <td>-0.248237</td>\n      <td>-0.046856</td>\n      <td>-0.366515</td>\n      <td>0.371055</td>\n      <td>0.387127</td>\n      <td>-0.086699</td>\n      <td>-0.266235</td>\n      <td>...</td>\n      <td>-0.101303</td>\n      <td>-0.187162</td>\n      <td>0.108514</td>\n      <td>-0.177368</td>\n      <td>0.260718</td>\n      <td>0.150828</td>\n      <td>-0.205036</td>\n      <td>-0.006340</td>\n      <td>0.436779</td>\n      <td>0.446601</td>\n    </tr>\n    <tr>\n      <th>967</th>\n      <td>0.311766</td>\n      <td>-0.014036</td>\n      <td>-0.040445</td>\n      <td>-0.234838</td>\n      <td>-0.193043</td>\n      <td>-0.164190</td>\n      <td>0.229046</td>\n      <td>0.279721</td>\n      <td>0.126862</td>\n      <td>-0.455163</td>\n      <td>...</td>\n      <td>-0.002532</td>\n      <td>-0.214854</td>\n      <td>0.003515</td>\n      <td>-0.244796</td>\n      <td>0.235992</td>\n      <td>0.053680</td>\n      <td>-0.070399</td>\n      <td>0.062612</td>\n      <td>0.513922</td>\n      <td>0.477407</td>\n    </tr>\n  </tbody>\n</table>\n<p>968 rows Ã— 768 columns</p>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(model_ckpt).to(device)\n",
    "\n",
    "def extract_features(batch):\n",
    "    \"\"\"Extract features from a batch of items\"\"\"\n",
    "    inputs = {k:v.to(device) for k, v in batch.items() if k in tokenizer.model_input_names}\n",
    "    with torch.no_grad():\n",
    "        last_hidden_state = model(**inputs).last_hidden_state\n",
    "        return {\"hidden_state\": last_hidden_state[:,0].cpu().numpy()}\n",
    "\n",
    "vaccine = vaccine.map(extract_features, batched=True, batch_size=8)\n",
    "embeds = pd.DataFrame(vaccine['hidden_state'])\n",
    "embeds"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-07T16:49:42.388881Z",
     "start_time": "2023-08-07T16:49:13.005824Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Predicting vaccine decisions with embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-07T16:49:45.296175Z",
     "start_time": "2023-08-07T16:49:44.728639Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08978435256497175\n"
     ]
    }
   ],
   "source": [
    "regr = RidgeCV()\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeds, vaccine['labels'], random_state=42)\n",
    "regr.fit(X_train, y_train)\n",
    "print(regr.score(X_test, y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-07T16:49:45.412561Z",
     "start_time": "2023-08-07T16:49:45.298008Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pedicting vaccine decisions the LM fine-tuning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import evaluate"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-07T16:50:00.342094Z",
     "start_time": "2023-08-07T16:50:00.283484Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['labels', 'text', '__index_level_0__', 'input_ids', 'attention_mask', 'hidden_state'],\n        num_rows: 774\n    })\n    test: Dataset({\n        features: ['labels', 'text', '__index_level_0__', 'input_ids', 'attention_mask', 'hidden_state'],\n        num_rows: 194\n    })\n})"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vaccine = vaccine.train_test_split(test_size=0.2)\n",
    "vaccine"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-07T16:50:01.170667Z",
     "start_time": "2023-08-07T16:50:01.161549Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": "DistilBertForSequenceClassification(\n  (distilbert): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0-5): 6 x TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n  (classifier): Linear(in_features=768, out_features=1, bias=True)\n  (dropout): Dropout(p=0.2, inplace=False)\n)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining the model\n",
    "model = (AutoModelForSequenceClassification\n",
    "         .from_pretrained(model_ckpt, num_labels=1) # num_labels=1 for regression\n",
    "         .to(device))\n",
    "\n",
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-07T16:50:05.373039Z",
     "start_time": "2023-08-07T16:50:04.366926Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# Training the model\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.load(\"r_squared\")\n",
    "    preds, labels = eval_preds\n",
    "    return {\"r_squared\": metric.compute(predictions=preds, references=labels)}\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=vaccine['train'],\n",
    "    eval_dataset=vaccine['test'],\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-07T16:50:20.068574Z",
     "start_time": "2023-08-07T16:50:20.044990Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhussain/opt/anaconda3/envs/LLM4JDM/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='291' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/291 : < :, Epoch 0.01/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "TrainOutput(global_step=291, training_loss=0.07256945056194292, metrics={'train_runtime': 189.2224, 'train_samples_per_second': 12.271, 'train_steps_per_second': 1.538, 'total_flos': 307583814260736.0, 'train_loss': 0.07256945056194292, 'epoch': 3.0})"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-07T16:53:30.808469Z",
     "start_time": "2023-08-07T16:50:21.573074Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
