{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Using Notebook Environments\n",
    "1. To run a cell, press ```shift + enter```. The notebook will execute the code in the cell and move to the next cell. If the cell contains a markdown cell, it will render the markdown and move to the next cell.\n",
    "2. Since cells can be executed in any order and variables can be over-written, you may at some point feel that you have lost track of the state of your notebook. If this is the case, you can always restart the kernel by clicking ```Runtime``` in the menu bar (if you're using Colab) and selecting ```Restart runtime```. This will clear all variables and outputs.\n",
    "3. The final variable in a cell will be printed to the screen. If you want to print multiple variables, you can use the ```print()``` function as usual.\n",
    "\n",
    "Notebook environments support code cells and markdown cells. For the purposes of this workshop, markdown cells are used to provide high-level explanations of the code. More specific details are provided in the code cells themselves in the form of comments (lines beginning with ```#```)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:  # If in Google Colab environment\n",
    "    # Installing requisite packages\n",
    "    !pip install datasets transformers evaluate\n",
    "    !pip install accelerate -U\n",
    "\n",
    "    # Mount google drive to enable access to data files\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # Change working directory to ex1\n",
    "    %cd /content/drive/MyDrive/LLM4JDM/ex1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-18T11:39:46.142146Z",
     "start_time": "2023-08-18T11:39:46.127894Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preparing data\n",
    "We begin by loading the requisite packages. For those coming from R, packages in Python are sometimes given shorter names for use in the code via the ```import <name> as <nickname>``` syntax (e.g. ```import pandas as pd```). These are usually standardized nicknames. We here make use of three packages:\n",
    "1. ```pandas```: A very popular library for reading and manipulating data in python.\n",
    "2. ```datasets```: A package from the HuggingFace (HF) ecosystem for loading and manipulating datasets in a format ready for use with HF models.\n",
    "3. ```transformers```: A package from the HF ecosystem for loading and manipulating transfomer-based models."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-18T11:39:48.595117Z",
     "start_time": "2023-08-18T11:39:47.300259Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The dataset, kindly provided by Aka & Bhatia ([2022](https://www.journals.uchicago.edu/doi/full/10.1086/718456])) takes the following structure:\n",
    "1. ```health_state```: A couple of word description of the health state (e.g. broken leg) \n",
    "2. ```content```: More in-depth explanation of the health state scraped from the NHS website.\n",
    "3. ```health_rating```: Average participant rating of the severity of the health state (larger rating -> less severe)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "               health_state   \n0                Broken leg  \\\n1                   Bulimia   \n2               Hyperacusis   \n3                       DVT   \n4         Ectopic pregnancy   \n..                      ...   \n772           Typhoid fever   \n773  Ankylosing spondylitis   \n774            Sleepwalking   \n775                    Fits   \n776    Diabetic retinopathy   \n\n                                               content  health_rating  \n0    \\nA broken leg (leg fracture) will be severely...      49.333333  \n1    \\nBulimia is an eating disorder and mental hea...      34.181818  \n2    \\nHyperacusis is when everyday sounds seem muc...      53.818182  \n3    DVT (deep vein thrombosis) is a blood clot in ...      12.800000  \n4    An ectopic pregnancy is when a fertilised egg ...      31.700000  \n..                                                 ...            ...  \n772  \\nTyphoid fever is a bacterial infection that ...      27.900000  \n773  Ankylosing spondylitis (AS) is a long-term con...      30.800000  \n774  Sleepwalking is when someone walks or carries ...      71.181818  \n775  If you see someone having a seizure or fit, th...      34.111111  \n776  Diabetic retinopathy is a complication of diab...      42.636364  \n\n[777 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>health_state</th>\n      <th>content</th>\n      <th>health_rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Broken leg</td>\n      <td>\\nA broken leg (leg fracture) will be severely...</td>\n      <td>49.333333</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Bulimia</td>\n      <td>\\nBulimia is an eating disorder and mental hea...</td>\n      <td>34.181818</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Hyperacusis</td>\n      <td>\\nHyperacusis is when everyday sounds seem muc...</td>\n      <td>53.818182</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>DVT</td>\n      <td>DVT (deep vein thrombosis) is a blood clot in ...</td>\n      <td>12.800000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Ectopic pregnancy</td>\n      <td>An ectopic pregnancy is when a fertilised egg ...</td>\n      <td>31.700000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>772</th>\n      <td>Typhoid fever</td>\n      <td>\\nTyphoid fever is a bacterial infection that ...</td>\n      <td>27.900000</td>\n    </tr>\n    <tr>\n      <th>773</th>\n      <td>Ankylosing spondylitis</td>\n      <td>Ankylosing spondylitis (AS) is a long-term con...</td>\n      <td>30.800000</td>\n    </tr>\n    <tr>\n      <th>774</th>\n      <td>Sleepwalking</td>\n      <td>Sleepwalking is when someone walks or carries ...</td>\n      <td>71.181818</td>\n    </tr>\n    <tr>\n      <th>775</th>\n      <td>Fits</td>\n      <td>If you see someone having a seizure or fit, th...</td>\n      <td>34.111111</td>\n    </tr>\n    <tr>\n      <th>776</th>\n      <td>Diabetic retinopathy</td>\n      <td>Diabetic retinopathy is a complication of diab...</td>\n      <td>42.636364</td>\n    </tr>\n  </tbody>\n</table>\n<p>777 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading in the .csv data\n",
    "dat = pd.read_csv('health.csv')\n",
    "dat"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-18T11:57:14.530599Z",
     "start_time": "2023-08-18T11:57:14.509474Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                  text     labels\n0    Broken leg. A broken leg (leg fracture) will b...  49.333333\n1    Bulimia. Bulimia is an eating disorder and men...  34.181818\n2    Hyperacusis. Hyperacusis is when everyday soun...  53.818182\n3    DVT. DVT (deep vein thrombosis) is a blood clo...  12.800000\n4    Ectopic pregnancy. An ectopic pregnancy is whe...  31.700000\n..                                                 ...        ...\n772  Typhoid fever. Typhoid fever is a bacterial in...  27.900000\n773  Ankylosing spondylitis. Ankylosing spondylitis...  30.800000\n774  Sleepwalking. Sleepwalking is when someone wal...  71.181818\n775  Fits. If you see someone having a seizure or f...  34.111111\n776  Diabetic retinopathy. Diabetic retinopathy is ...  42.636364\n\n[777 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Broken leg. A broken leg (leg fracture) will b...</td>\n      <td>49.333333</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Bulimia. Bulimia is an eating disorder and men...</td>\n      <td>34.181818</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Hyperacusis. Hyperacusis is when everyday soun...</td>\n      <td>53.818182</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>DVT. DVT (deep vein thrombosis) is a blood clo...</td>\n      <td>12.800000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Ectopic pregnancy. An ectopic pregnancy is whe...</td>\n      <td>31.700000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>772</th>\n      <td>Typhoid fever. Typhoid fever is a bacterial in...</td>\n      <td>27.900000</td>\n    </tr>\n    <tr>\n      <th>773</th>\n      <td>Ankylosing spondylitis. Ankylosing spondylitis...</td>\n      <td>30.800000</td>\n    </tr>\n    <tr>\n      <th>774</th>\n      <td>Sleepwalking. Sleepwalking is when someone wal...</td>\n      <td>71.181818</td>\n    </tr>\n    <tr>\n      <th>775</th>\n      <td>Fits. If you see someone having a seizure or f...</td>\n      <td>34.111111</td>\n    </tr>\n    <tr>\n      <th>776</th>\n      <td>Diabetic retinopathy. Diabetic retinopathy is ...</td>\n      <td>42.636364</td>\n    </tr>\n  </tbody>\n</table>\n<p>777 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenating the health_state and content descriptions into single column\n",
    "dat['text'] = dat['health_state'] + '. ' + dat['content']\n",
    "\n",
    "# Cleaning new line characters from the data\n",
    "dat['text'] = dat['text'].str.replace('\\n', '')\n",
    "\n",
    "\n",
    "dat = (\n",
    "    dat[['text', 'health_rating']]  # Selecting the columns we need\n",
    "    .rename(columns={'health_rating': 'labels'})  # renaming them to standardised names (text, labels)\n",
    ")\n",
    "\n",
    "dat"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-18T11:59:33.047937Z",
     "start_time": "2023-08-18T11:59:33.043719Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['text', 'labels'],\n    num_rows: 777\n})"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert pandas dataframe to HF Dataset\n",
    "dat = Dataset.from_pandas(dat)\n",
    "dat"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-18T11:59:36.408627Z",
     "start_time": "2023-08-18T11:59:36.378649Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Features of the ```Dataset``` object can be accessed like keys in a dictionary, and behave like python lists. Sample can be accessed by index, returning a dictionary where keys correspond to feature names. This reflects the fact that ```Datasets``` is based on Apache Arrow, which defines a typed columnar format that is more memory efficient than native Python"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "{'text': 'Broken leg. A broken leg (leg fracture) will be severely painful and may be\\xa0swollen or bruised. You usually will not be able to walk on it.If it\\'s a severe fracture, the leg may be an odd shape and the bone may even be poking out of the skin. There may have been a \"crack\" sound when the leg was broken, and the shock and pain of breaking your leg may cause you to feel faint, dizzy or sick.',\n 'labels': 49.33333333}"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-18T11:59:38.684581Z",
     "start_time": "2023-08-18T11:59:38.676043Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To use models in the HF ecosystem, one must first define a model checkpoint (```ckpt```): the set of weights that are loaded into a given transformer architecture. This often needs to be done well before we even initialise the model, since data preprocessing steps such as tokenization must follow the steps used to train the original model. We just need a pretrained base model for our purposes (i.e. one that has not yet been fine-tuned on a specific task). One popular lightweight option is ```distilbert-base-uncased```."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "# Defining model checkpoint\n",
    "model_ckpt = 'distilbert-base-uncased'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-18T11:59:43.703554Z",
     "start_time": "2023-08-18T11:59:43.696423Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tokenization is the process of breaking raw text into the desired atomic units for one's modelling task. This may be as simple as splitting the text into individual characters. In the case of transformer-based models, tokenization is a bit more complex, usually occurring at the sub-word level."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 30522, max context length: 512\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing the dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "print(f'Vocabulary size: {tokenizer.vocab_size}, max context length: {tokenizer.model_max_length}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-18T11:59:45.598873Z",
     "start_time": "2023-08-18T11:59:45.364585Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Two important arguments relating to tokenization:\n",
    "1. ```padding```: Used to fill up sequences to a certain length, ensuring that all sequences in a batch have the same length. This is essential for training and inference with deep learning models that operate on fixed-size input tensors.\n",
    "2. ```truncation```: Truncation is the process of cutting off parts of the sequence to ensure that it fits within a specified maximum length (e.g. 512 tokens for BERT models)\n",
    "\n",
    "The combination of padding and truncation ensures that all sequences in a batch conform to a consistent, fixed size, which is essential for processing in parallel on modern hardware like GPUs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/777 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "967c41f51eb349c8a31076f1aaf7d11b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "{'text': 'Broken leg. A broken leg (leg fracture) will be severely painful and may be\\xa0swollen or bruised. You usually will not be able to walk on it.If it\\'s a severe fracture, the leg may be an odd shape and the bone may even be poking out of the skin. There may have been a \"crack\" sound when the leg was broken, and the shock and pain of breaking your leg may cause you to feel faint, dizzy or sick.',\n 'labels': 49.33333333,\n 'input_ids': [101,\n  3714,\n  4190,\n  1012,\n  1037,\n  3714,\n  4190,\n  1006,\n  4190,\n  19583,\n  1007,\n  2097,\n  2022,\n  8949,\n  9145,\n  1998,\n  2089,\n  2022,\n  13408,\n  2030,\n  18618,\n  1012,\n  2017,\n  2788,\n  2097,\n  2025,\n  2022,\n  2583,\n  2000,\n  3328,\n  2006,\n  2009,\n  1012,\n  2065,\n  2009,\n  1005,\n  1055,\n  1037,\n  5729,\n  19583,\n  1010,\n  1996,\n  4190,\n  2089,\n  2022,\n  2019,\n  5976,\n  4338,\n  1998,\n  1996,\n  5923,\n  2089,\n  2130,\n  2022,\n  21603,\n  2041,\n  1997,\n  1996,\n  3096,\n  1012,\n  2045,\n  2089,\n  2031,\n  2042,\n  1037,\n  1000,\n  8579,\n  1000,\n  2614,\n  2043,\n  1996,\n  4190,\n  2001,\n  3714,\n  1010,\n  1998,\n  1996,\n  5213,\n  1998,\n  3255,\n  1997,\n  4911,\n  2115,\n  4190,\n  2089,\n  3426,\n  2017,\n  2000,\n  2514,\n  8143,\n  1010,\n  14849,\n  2030,\n  5305,\n  1012,\n  102,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0],\n 'attention_mask': [1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0]}"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to tokenize a batch of samples\n",
    "batch_tokenize = lambda batch: tokenizer(batch['text'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "#  Tokenizing the dataset\n",
    "dat = dat.map(batch_tokenize, batched=True)\n",
    "dat[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-18T11:59:48.066791Z",
     "start_time": "2023-08-18T11:59:47.933845Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Looking at the first sample, we see some important special tokens:\n",
    "1. ```[CLS]```: Often added at the beginning of the input sequence. In the context of classification tasks, the embedding corresponding to the [CLS] token (after passing through the model) is often used as the aggregate representation for the entire sequence.\n",
    "2. ```[SEP]```: Used to separate different segments in a sequence. For example, in tasks that take two different sentences as input (such as question-answering or text-pair classification), the [SEP] token is placed between the two sentences to indicate that they are distinct segments. This helps the model understand and process the two segments appropriately, recognizing the boundaries between them."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "['[CLS]',\n 'broken',\n 'leg',\n '.',\n 'a',\n 'broken',\n 'leg',\n '(',\n 'leg',\n 'fracture',\n ')',\n 'will',\n 'be',\n 'severely',\n 'painful',\n 'and',\n 'may',\n 'be',\n 'swollen',\n 'or',\n 'bruised',\n '.',\n 'you',\n 'usually',\n 'will',\n 'not',\n 'be',\n 'able',\n 'to',\n 'walk',\n 'on',\n 'it',\n '.',\n 'if',\n 'it',\n \"'\",\n 's',\n 'a',\n 'severe',\n 'fracture',\n ',',\n 'the',\n 'leg',\n 'may',\n 'be',\n 'an',\n 'odd',\n 'shape',\n 'and',\n 'the',\n 'bone',\n 'may',\n 'even',\n 'be',\n 'poking',\n 'out',\n 'of',\n 'the',\n 'skin',\n '.',\n 'there',\n 'may',\n 'have',\n 'been',\n 'a',\n '\"',\n 'crack',\n '\"',\n 'sound',\n 'when',\n 'the',\n 'leg',\n 'was',\n 'broken',\n ',',\n 'and',\n 'the',\n 'shock',\n 'and',\n 'pain',\n 'of',\n 'breaking',\n 'your',\n 'leg',\n 'may',\n 'cause',\n 'you',\n 'to',\n 'feel',\n 'faint',\n ',',\n 'dizzy',\n 'or',\n 'sick',\n '.',\n '[SEP]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]']"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspecting tokenization by looking at the first 30 tokens of the first sample\n",
    "tokenizer.convert_ids_to_tokens(dat[0]['input_ids'])[:100]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-18T12:00:00.195277Z",
     "start_time": "2023-08-18T12:00:00.186179Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature Extraction\n",
    "We require two packages/modules for feature extraction:\n",
    "1. ```torch```: The PyTorch package, which is the most popular deep learning framework amongst researchers (https://paperswithcode.com/trends).\n",
    "2. ```Automodel```: A module from the HF ecosystem that allows us to load a pretrained model and use it for inference. This is a very convenient way to use pretrained models, since it abstracts away the details of the model architecture and allows us to focus on the task at hand."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(42) # For reproducibility\n",
    "from transformers import AutoModel"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-18T12:00:15.946201Z",
     "start_time": "2023-08-18T12:00:15.936774Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In order to pass our data to the model, we need to convert it to torch tensors. If you are familiar with NumPy, torch tensors are very similar, but with the added benefit of being able to run on GPUs (which are optimized for tensor operations)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['text', 'labels', 'input_ids', 'attention_mask'],\n    num_rows: 777\n})"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "dat"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-18T12:00:16.181357Z",
     "start_time": "2023-08-18T12:00:16.177035Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='mps')"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the model and moving it to the GPU if available\n",
    "if torch.cuda.is_available():  # for nvidia GPUs etc.\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available(): # for Apple Metal Performance Sharder (mps) GPUs\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-18T12:00:16.277122Z",
     "start_time": "2023-08-18T12:00:16.272872Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": "\"Model inputs: ['input_ids', 'attention_mask']\""
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading distilbert-base-uncased and moving it to the GPU if available\n",
    "model = AutoModel.from_pretrained(model_ckpt).to(device)\n",
    "f'Model inputs: {tokenizer.model_input_names}'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-18T12:00:17.498341Z",
     "start_time": "2023-08-18T12:00:16.376464Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Extracting features from the model is as simple as passing the data to the model and extracting the last hidden state (the activations of the final layer of the model). We here extract the hidden state for the [CLS] token, which is often used as the aggregate representation for the entire sequence."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/777 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3e6c12ce3c5e4c7b8dc4b375fa662585"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "torch.Size([777, 768])"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_features(batch):\n",
    "    # Each batch is a dictionary with keys corresponding to the feature names. We only need the input ids and attention masks\n",
    "    inputs = {k:v.to(device) for k, v in batch.items() if k in tokenizer.model_input_names}\n",
    "\n",
    "    # Tell torch not to build the computation graph during inference with `torch.no_grad()`\n",
    "    with torch.no_grad():\n",
    "        last_hidden_state = model(**inputs).last_hidden_state # Extract last hidden states\n",
    "\n",
    "    # Return vector for [CLS] token\n",
    "    return {\"hidden_state\": last_hidden_state[:,0].cpu().numpy()}\n",
    "\n",
    "# Extracting features. Features are extracted in batches of 8 samples to avoid running out of memory.\n",
    "dat = dat.map(extract_features, batched=True, batch_size=8)\n",
    "dat['hidden_state'].shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-18T12:00:41.967231Z",
     "start_time": "2023-08-18T12:00:17.501958Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Predicting health perception with features\n",
    "To predict the decisions using the extracted features and evaluate prediction performance we will make use of ```sklearn``` (scikit-learn), a general machine learning library. Since we are dealing with high-dimensinoal embeddings, ordinary least squares regression runs a risk of overfitting. Instead, we will use a regularized (linear) regression model (```RidgeCV```). We evaluate model performance on a holdout test set using the coefficient of determination ($R^2$)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-18T11:48:31.546964Z",
     "start_time": "2023-08-18T11:48:30.659665Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "          0         1         2         3         4         5         6     \n0   -0.148475 -0.093959 -0.072606 -0.116719 -0.197698 -0.017892  0.583106  \\\n1   -0.188410 -0.126732 -0.108507 -0.293917 -0.218685  0.010479  0.353766   \n2   -0.171668 -0.050836 -0.194084 -0.312468 -0.174100 -0.011414  0.494835   \n3   -0.338618 -0.057856  0.033038 -0.357180 -0.202168  0.036766  0.324142   \n4   -0.098636 -0.401457 -0.182545 -0.179665  0.064003  0.001181  0.291596   \n..        ...       ...       ...       ...       ...       ...       ...   \n772 -0.062923 -0.015915 -0.222807 -0.208136 -0.150941 -0.284561  0.377012   \n773 -0.279977 -0.197014  0.098390 -0.303245 -0.296704  0.008300  0.288116   \n774 -0.073572 -0.369269  0.076614 -0.097476 -0.064454 -0.054622  0.647633   \n775 -0.181972 -0.083371  0.113803 -0.065226  0.033920 -0.130653  0.366495   \n776 -0.148871  0.094196  0.097353 -0.295030  0.079687  0.020537  0.388873   \n\n          7         8         9    ...       758       759       760   \n0    0.199220 -0.294927 -0.524650  ...  0.132813 -0.284638  0.341142  \\\n1    0.414596 -0.126797 -0.443795  ... -0.127295 -0.445322  0.027356   \n2    0.385607 -0.115327 -0.769145  ...  0.038155 -0.398147  0.033521   \n3    0.252396 -0.024016 -0.509216  ...  0.136020 -0.368171  0.198924   \n4    0.541527 -0.020205 -0.360373  ... -0.063380 -0.327838  0.088122   \n..        ...       ...       ...  ...       ...       ...       ...   \n772  0.315895 -0.106533 -0.588246  ...  0.030073 -0.439836  0.044907   \n773  0.194376 -0.128856 -0.353388  ... -0.026887 -0.458937  0.206063   \n774  0.325533 -0.363346 -0.434460  ... -0.008730 -0.349660  0.011828   \n775  0.114534 -0.174858 -0.477767  ... -0.016639 -0.359916  0.018970   \n776  0.140645 -0.075444 -0.502733  ... -0.127892 -0.346318 -0.005479   \n\n          761       762       763       764       765       766       767  \n0   -0.053502  0.222674  0.264824 -0.127856  0.016527 -0.165402  0.297364  \n1   -0.416753 -0.055470  0.358791 -0.196915 -0.088872 -0.025871  0.366596  \n2   -0.147881  0.203515  0.511178 -0.293286 -0.232812 -0.022038  0.263111  \n3   -0.247393  0.010923  0.367897 -0.227436 -0.007300  0.000287  0.664291  \n4   -0.291927 -0.019845  0.092592  0.024323 -0.213633  0.043224  0.738061  \n..        ...       ...       ...       ...       ...       ...       ...  \n772 -0.227704 -0.230707  0.228135 -0.225799  0.025129  0.063843  0.814006  \n773 -0.471932  0.051058  0.154827 -0.420223  0.196191  0.028353  0.592685  \n774 -0.398827 -0.018842  0.212129  0.035592 -0.216679 -0.086199  0.413299  \n775 -0.175317  0.033933  0.393584 -0.225063 -0.109431  0.165128  0.303420  \n776 -0.243680  0.020777  0.164765 -0.174303 -0.099681 -0.098134  0.406878  \n\n[777 rows x 768 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>758</th>\n      <th>759</th>\n      <th>760</th>\n      <th>761</th>\n      <th>762</th>\n      <th>763</th>\n      <th>764</th>\n      <th>765</th>\n      <th>766</th>\n      <th>767</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.148475</td>\n      <td>-0.093959</td>\n      <td>-0.072606</td>\n      <td>-0.116719</td>\n      <td>-0.197698</td>\n      <td>-0.017892</td>\n      <td>0.583106</td>\n      <td>0.199220</td>\n      <td>-0.294927</td>\n      <td>-0.524650</td>\n      <td>...</td>\n      <td>0.132813</td>\n      <td>-0.284638</td>\n      <td>0.341142</td>\n      <td>-0.053502</td>\n      <td>0.222674</td>\n      <td>0.264824</td>\n      <td>-0.127856</td>\n      <td>0.016527</td>\n      <td>-0.165402</td>\n      <td>0.297364</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.188410</td>\n      <td>-0.126732</td>\n      <td>-0.108507</td>\n      <td>-0.293917</td>\n      <td>-0.218685</td>\n      <td>0.010479</td>\n      <td>0.353766</td>\n      <td>0.414596</td>\n      <td>-0.126797</td>\n      <td>-0.443795</td>\n      <td>...</td>\n      <td>-0.127295</td>\n      <td>-0.445322</td>\n      <td>0.027356</td>\n      <td>-0.416753</td>\n      <td>-0.055470</td>\n      <td>0.358791</td>\n      <td>-0.196915</td>\n      <td>-0.088872</td>\n      <td>-0.025871</td>\n      <td>0.366596</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.171668</td>\n      <td>-0.050836</td>\n      <td>-0.194084</td>\n      <td>-0.312468</td>\n      <td>-0.174100</td>\n      <td>-0.011414</td>\n      <td>0.494835</td>\n      <td>0.385607</td>\n      <td>-0.115327</td>\n      <td>-0.769145</td>\n      <td>...</td>\n      <td>0.038155</td>\n      <td>-0.398147</td>\n      <td>0.033521</td>\n      <td>-0.147881</td>\n      <td>0.203515</td>\n      <td>0.511178</td>\n      <td>-0.293286</td>\n      <td>-0.232812</td>\n      <td>-0.022038</td>\n      <td>0.263111</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.338618</td>\n      <td>-0.057856</td>\n      <td>0.033038</td>\n      <td>-0.357180</td>\n      <td>-0.202168</td>\n      <td>0.036766</td>\n      <td>0.324142</td>\n      <td>0.252396</td>\n      <td>-0.024016</td>\n      <td>-0.509216</td>\n      <td>...</td>\n      <td>0.136020</td>\n      <td>-0.368171</td>\n      <td>0.198924</td>\n      <td>-0.247393</td>\n      <td>0.010923</td>\n      <td>0.367897</td>\n      <td>-0.227436</td>\n      <td>-0.007300</td>\n      <td>0.000287</td>\n      <td>0.664291</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.098636</td>\n      <td>-0.401457</td>\n      <td>-0.182545</td>\n      <td>-0.179665</td>\n      <td>0.064003</td>\n      <td>0.001181</td>\n      <td>0.291596</td>\n      <td>0.541527</td>\n      <td>-0.020205</td>\n      <td>-0.360373</td>\n      <td>...</td>\n      <td>-0.063380</td>\n      <td>-0.327838</td>\n      <td>0.088122</td>\n      <td>-0.291927</td>\n      <td>-0.019845</td>\n      <td>0.092592</td>\n      <td>0.024323</td>\n      <td>-0.213633</td>\n      <td>0.043224</td>\n      <td>0.738061</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>772</th>\n      <td>-0.062923</td>\n      <td>-0.015915</td>\n      <td>-0.222807</td>\n      <td>-0.208136</td>\n      <td>-0.150941</td>\n      <td>-0.284561</td>\n      <td>0.377012</td>\n      <td>0.315895</td>\n      <td>-0.106533</td>\n      <td>-0.588246</td>\n      <td>...</td>\n      <td>0.030073</td>\n      <td>-0.439836</td>\n      <td>0.044907</td>\n      <td>-0.227704</td>\n      <td>-0.230707</td>\n      <td>0.228135</td>\n      <td>-0.225799</td>\n      <td>0.025129</td>\n      <td>0.063843</td>\n      <td>0.814006</td>\n    </tr>\n    <tr>\n      <th>773</th>\n      <td>-0.279977</td>\n      <td>-0.197014</td>\n      <td>0.098390</td>\n      <td>-0.303245</td>\n      <td>-0.296704</td>\n      <td>0.008300</td>\n      <td>0.288116</td>\n      <td>0.194376</td>\n      <td>-0.128856</td>\n      <td>-0.353388</td>\n      <td>...</td>\n      <td>-0.026887</td>\n      <td>-0.458937</td>\n      <td>0.206063</td>\n      <td>-0.471932</td>\n      <td>0.051058</td>\n      <td>0.154827</td>\n      <td>-0.420223</td>\n      <td>0.196191</td>\n      <td>0.028353</td>\n      <td>0.592685</td>\n    </tr>\n    <tr>\n      <th>774</th>\n      <td>-0.073572</td>\n      <td>-0.369269</td>\n      <td>0.076614</td>\n      <td>-0.097476</td>\n      <td>-0.064454</td>\n      <td>-0.054622</td>\n      <td>0.647633</td>\n      <td>0.325533</td>\n      <td>-0.363346</td>\n      <td>-0.434460</td>\n      <td>...</td>\n      <td>-0.008730</td>\n      <td>-0.349660</td>\n      <td>0.011828</td>\n      <td>-0.398827</td>\n      <td>-0.018842</td>\n      <td>0.212129</td>\n      <td>0.035592</td>\n      <td>-0.216679</td>\n      <td>-0.086199</td>\n      <td>0.413299</td>\n    </tr>\n    <tr>\n      <th>775</th>\n      <td>-0.181972</td>\n      <td>-0.083371</td>\n      <td>0.113803</td>\n      <td>-0.065226</td>\n      <td>0.033920</td>\n      <td>-0.130653</td>\n      <td>0.366495</td>\n      <td>0.114534</td>\n      <td>-0.174858</td>\n      <td>-0.477767</td>\n      <td>...</td>\n      <td>-0.016639</td>\n      <td>-0.359916</td>\n      <td>0.018970</td>\n      <td>-0.175317</td>\n      <td>0.033933</td>\n      <td>0.393584</td>\n      <td>-0.225063</td>\n      <td>-0.109431</td>\n      <td>0.165128</td>\n      <td>0.303420</td>\n    </tr>\n    <tr>\n      <th>776</th>\n      <td>-0.148871</td>\n      <td>0.094196</td>\n      <td>0.097353</td>\n      <td>-0.295030</td>\n      <td>0.079687</td>\n      <td>0.020537</td>\n      <td>0.388873</td>\n      <td>0.140645</td>\n      <td>-0.075444</td>\n      <td>-0.502733</td>\n      <td>...</td>\n      <td>-0.127892</td>\n      <td>-0.346318</td>\n      <td>-0.005479</td>\n      <td>-0.243680</td>\n      <td>0.020777</td>\n      <td>0.164765</td>\n      <td>-0.174303</td>\n      <td>-0.099681</td>\n      <td>-0.098134</td>\n      <td>0.406878</td>\n    </tr>\n  </tbody>\n</table>\n<p>777 rows × 768 columns</p>\n</div>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting features to a pandas dataframe for compatibility with sklearn\n",
    "embeds = pd.DataFrame(dat['hidden_state'])\n",
    "embeds"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-18T11:48:32.005297Z",
     "start_time": "2023-08-18T11:48:31.978166Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "RidgeCV()",
      "text/html": "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RidgeCV()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RidgeCV</label><div class=\"sk-toggleable__content\"><pre>RidgeCV()</pre></div></div></div></div></div>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiating the RidgeCV model\n",
    "regr = RidgeCV()\n",
    "regr"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-18T11:48:34.139777Z",
     "start_time": "2023-08-18T11:48:34.127462Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "'Train size: 582, test size: 195'"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeds, dat['labels'], random_state=42)\n",
    "f'Train size: {len(X_train)}, test size: {len(X_test)}'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-18T11:48:35.959976Z",
     "start_time": "2023-08-18T11:48:35.946585Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "'Test R2 = 0.57'"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the model and evaluating performance\n",
    "regr.fit(X_train, y_train)\n",
    "f'Test R2 = {regr.score(X_test, y_test).round(2)}'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-18T11:48:37.931558Z",
     "start_time": "2023-08-18T11:48:37.841186Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Exercise*: Feel free to try out different regression models (e.g. LassoCV): https://scikit-learn.org/stable/supervised_learning.html"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pedicting health perception with LM fine-tuning\n",
    "We here make use of three modules from the transformers library:\n",
    "1. ```AutoModelForSequenceClassification```: Loads a pretrained model ready for fine-tuning it on sequence classification/regression labels.\n",
    "2. ```TrainingArguments```: Specify training arguments such as the number of epochs, batch size, learning rate, etc.\n",
    "3. ```Trainer```: Allows us to train a model using the training arguments and a training dataset.\n",
    "\n",
    "We also employ the ```evaluate``` library to compute the coefficient of determination ($R^2$) on the test set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import evaluate"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-18T12:01:03.983578Z",
     "start_time": "2023-08-18T12:01:03.951513Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['text', 'labels', 'input_ids', 'attention_mask', 'hidden_state'],\n        num_rows: 621\n    })\n    test: Dataset({\n        features: ['text', 'labels', 'input_ids', 'attention_mask', 'hidden_state'],\n        num_rows: 156\n    })\n})"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting the data into train and test sets\n",
    "dat = dat.train_test_split(test_size=0.2)\n",
    "dat"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-18T12:01:04.277018Z",
     "start_time": "2023-08-18T12:01:04.264065Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The main difference here from the model we used for feature extraction is ```distilbert-base-uncased``` now has a classification/regression head attached. You will see a warning that some parts of the model are randomly initialized. This is normal since the head has not yet been trained."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": "DistilBertForSequenceClassification(\n  (distilbert): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0-5): 6 x TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n  (classifier): Linear(in_features=768, out_features=1, bias=True)\n  (dropout): Dropout(p=0.2, inplace=False)\n)"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading distilbert-base-uncased and moving it to the GPU if available\n",
    "model = (AutoModelForSequenceClassification\n",
    "         .from_pretrained(model_ckpt, num_labels=1) # num_labels=1 for regression\n",
    "         .to(device))\n",
    "\n",
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-18T12:01:08.342959Z",
     "start_time": "2023-08-18T12:01:07.083296Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhussain/opt/anaconda3/envs/LLM4JDM/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='78' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 2/78 : < :, Epoch 0.01/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setting up training arguments for the trainer\n",
    "model_name = f\"{model_ckpt}-finetuned-health\"\n",
    "batch_size = 8\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_name,  # output directory to save training checkpoints\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    logging_steps=1/batch_size, # log training metrics at every epoch\n",
    "    evaluation_strategy=\"epoch\", # evaluate at the end of every epoch\n",
    "    num_train_epochs=1, # number of times to iterate over the training data\n",
    ")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    \"\"\"Computes the coefficient of determination (R2) on the test set\"\"\"\n",
    "    metric = evaluate.load(\"r_squared\")\n",
    "    preds, labels = eval_preds\n",
    "    return {\"r_squared\": metric.compute(predictions=preds, references=labels)}\n",
    "\n",
    "\n",
    "# Instantiating the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dat['train'],\n",
    "    eval_dataset=dat['test'],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Training the model\n",
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-08-18T12:01:16.036004Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
